[
  {
    "objectID": "notes/expfam/expfam.html",
    "href": "notes/expfam/expfam.html",
    "title": "The Exponential Family",
    "section": "",
    "text": "The classical motivation for exponential family comes from the principle of maximum entropy. Suppose we are given a random sample \\(\\{X_{1}, X_{2}, \\cdots, X_{n}\\}\\) from some distribution, and we compute the empirical expectations of certain functions that we choose: \\[\\hat{\\mu}_{i}=\\dfrac{1}{n}\\sum\\limits_{j=1}^{n}T_{i}(X_{j}) \\quad \\text{for } i \\in \\{1, \\cdots, s\\}\\] Based on these empirical expectations we want to infer a full probability distribution on the samples. A distribution \\(p\\) is said to be consistent 1 with the data we observe if: \\[\n\\hat{\\mu}_{i} = \\mathbb{E}_{p}[T_{i}(X_{i})] \\quad \\text{for } i \\in \\{1, \\cdots, s\\}.\n\\] We want to pick a consistent distribution but there are infinitely many distributions to choose from. The principle of maximum entropy suggests to pick the distribution with the largest entropy 2, which is defined as: \\[\nH(p) = - \\int p(x) \\log p(x) dx\n\\] Formally, we could pose our problem as finding a distribution \\(p^{*}\\) such that: \\[\np^{*} = \\underset{p}{\\text{argmax}} H(p)\n\\] Solving this problem, we get an answer that is of the form 3: \\[\np^{*} = \\exp \\left[\\sum\\limits_{i=1}^{s}\\theta_{i}T_{i}(x) - A(\\theta)\\right]h(x)\n\\] This form motivates the exponential families. # The Exponential Family\n\n\nThe exponential family of probability distributions is defined as a set of distributions whose density can be written in the following form: \\[\np(x \\vert \\eta) = h(x)\\exp\\{\\eta^{T}\\mathcal{T}(x) - \\mathcal{A}(\\eta)\\},\n\\] where: - \\(\\eta\\) is often referred to as natural or canonical parameters - \\(\\mathcal{T}(x)\\) is the sufficient statistic, because the likelihood for \\(x\\) only depends on \\(\\eta\\) and \\(\\mathcal{T}(x)\\) transforms \\(x\\) into sufficient statistic, which is something that summarizes the data well. - \\(\\mathcal{A}(\\eta)\\) is known as the cumulant function, or log-partition function and can be viewed as the logarithm of the normalization factor, i.e.: \\(\\mathcal{A}(\\eta) = \\log \\int h(x) \\exp \\{\\eta^{T}\\mathcal{T}(x)\\}dx\\)\nSometimes, the exponential family can also be represented as: \\[\np(x \\vert \\eta) = h(x) g(\\eta) \\exp \\{\\eta^{T}\\mathcal{T}(x)\\}\n\\] where the \\(g(\\eta)\\) can be interpreted as the coefficient that ensures the distribution is normalized, i.e.: \\[\ng(\\eta) = \\int h(x) \\exp\\{\\eta^{T}\\mathcal{T}(x)\\}\n\\] ## Examples\nThe simplest possible distribution that belongs to the exponential family is the Bernoulli distribution. Recall that we can write the Bernoulli distribution as: \\[\np(x \\vert \\mu) = \\text{Bern}(x \\vert \\mu) = \\mu^{x}(1-\\mu)^{1-x}\n\\] which we can rewrite as: \\[\np(x \\vert \\mu) = \\exp\\{x \\ln \\mu + (1-x)\\ln (1-\\mu)\\} = (1-\\mu) \\exp \\left\\{x\\ln \\left(\\dfrac{\\mu}{1-\\mu}\\right) + \\ln (1-\\mu)\\right\\}  \n\\] We can therefore see that the Bernoulli distribution is in the exponential family distribution where \\(\\mathcal{T}(x) = x; \\eta = \\ln \\left(\\dfrac{\\mu}{1-\\mu}\\right)\\). One interesting fact is that when we solve for \\(\\mu\\), we obtain the well-known logistic function. The derivation is as follows: \\[\n\\begin{aligned}\n\\eta &= \\ln \\left(\\dfrac{\\mu}{1-\\mu}\\right) && (\\text{derived from rewriting Bernoulli distribution})\\\\\n& \\rightarrow \\exp(\\eta) = \\dfrac{\\mu}{1-\\mu} && \\text{(taking exp on both sides)}\\\\\n& \\rightarrow \\mu = \\dfrac{\\exp(\\eta)}{1+\\exp(\\eta)} && \\text{(by basic algebra)}\\\\\n& \\rightarrow \\mu = \\dfrac{1}{1+\\exp(-\\eta)} && \\text{(multiplying by $\\exp(-\\mu)$)}\n\\end{aligned}\n\\] There are many common probability distribution that is in the exponential family. The table below listed some of them.\n\n\n\n\n\n\n\n\nDistribution\nProbability Density / Mass Function\n(\\(x\\)) Values\n\n\n\n\nGaussian\n\\[ p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\]\n\\[ x \\in \\mathbb{R} \\]\n\n\nBernoulli\n\\[ p(x) = \\alpha^x (1 - \\alpha)^{1 - x} \\]\n\\[ x \\in \\{0, 1\\} \\]\n\n\nBinomial\n\\[ p(x) = \\binom{n}{x} \\alpha^x (1 - \\alpha)^{n - x} \\]\n\\[ x \\in \\{0, 1, 2, \\ldots, n\\} \\]\n\n\nMultinomial\n\\[ p(x) = \\frac{n!}{x_1! x_2! \\ldots x_n!} \\prod_{i=1}^{n} \\alpha_i^{x_i} \\]\n\\[ x_i \\in \\{0, 1, 2, \\ldots, n\\}, \\sum x_i = n \\]\n\n\nExponential\n\\[ p(x) = \\lambda e^{-\\lambda x} \\]\n\\[ x \\in \\mathbb{R}^+ \\]\n\n\nPoisson\n\\[ p(x) = \\frac{e^{-\\lambda} \\lambda^x}{x!} \\]\n\\[ x \\in \\{0, 1, 2, \\ldots \\} \\]\n\n\nDirichlet\n\\[ p(x) = \\frac{\\Gamma \\left( \\sum_i \\alpha_i \\right)}{\\prod_i \\Gamma(\\alpha_i)} \\prod_i x_i^{\\alpha_i - 1} \\]\n\\[ x_i \\in [0, 1], \\sum x_i = 1 \\]"
  },
  {
    "objectID": "notes/expfam/expfam.html#definition",
    "href": "notes/expfam/expfam.html#definition",
    "title": "The Exponential Family",
    "section": "",
    "text": "The exponential family of probability distributions is defined as a set of distributions whose density can be written in the following form: \\[\np(x \\vert \\eta) = h(x)\\exp\\{\\eta^{T}\\mathcal{T}(x) - \\mathcal{A}(\\eta)\\},\n\\] where: - \\(\\eta\\) is often referred to as natural or canonical parameters - \\(\\mathcal{T}(x)\\) is the sufficient statistic, because the likelihood for \\(x\\) only depends on \\(\\eta\\) and \\(\\mathcal{T}(x)\\) transforms \\(x\\) into sufficient statistic, which is something that summarizes the data well. - \\(\\mathcal{A}(\\eta)\\) is known as the cumulant function, or log-partition function and can be viewed as the logarithm of the normalization factor, i.e.: \\(\\mathcal{A}(\\eta) = \\log \\int h(x) \\exp \\{\\eta^{T}\\mathcal{T}(x)\\}dx\\)\nSometimes, the exponential family can also be represented as: \\[\np(x \\vert \\eta) = h(x) g(\\eta) \\exp \\{\\eta^{T}\\mathcal{T}(x)\\}\n\\] where the \\(g(\\eta)\\) can be interpreted as the coefficient that ensures the distribution is normalized, i.e.: \\[\ng(\\eta) = \\int h(x) \\exp\\{\\eta^{T}\\mathcal{T}(x)\\}\n\\] ## Examples\nThe simplest possible distribution that belongs to the exponential family is the Bernoulli distribution. Recall that we can write the Bernoulli distribution as: \\[\np(x \\vert \\mu) = \\text{Bern}(x \\vert \\mu) = \\mu^{x}(1-\\mu)^{1-x}\n\\] which we can rewrite as: \\[\np(x \\vert \\mu) = \\exp\\{x \\ln \\mu + (1-x)\\ln (1-\\mu)\\} = (1-\\mu) \\exp \\left\\{x\\ln \\left(\\dfrac{\\mu}{1-\\mu}\\right) + \\ln (1-\\mu)\\right\\}  \n\\] We can therefore see that the Bernoulli distribution is in the exponential family distribution where \\(\\mathcal{T}(x) = x; \\eta = \\ln \\left(\\dfrac{\\mu}{1-\\mu}\\right)\\). One interesting fact is that when we solve for \\(\\mu\\), we obtain the well-known logistic function. The derivation is as follows: \\[\n\\begin{aligned}\n\\eta &= \\ln \\left(\\dfrac{\\mu}{1-\\mu}\\right) && (\\text{derived from rewriting Bernoulli distribution})\\\\\n& \\rightarrow \\exp(\\eta) = \\dfrac{\\mu}{1-\\mu} && \\text{(taking exp on both sides)}\\\\\n& \\rightarrow \\mu = \\dfrac{\\exp(\\eta)}{1+\\exp(\\eta)} && \\text{(by basic algebra)}\\\\\n& \\rightarrow \\mu = \\dfrac{1}{1+\\exp(-\\eta)} && \\text{(multiplying by $\\exp(-\\mu)$)}\n\\end{aligned}\n\\] There are many common probability distribution that is in the exponential family. The table below listed some of them.\n\n\n\n\n\n\n\n\nDistribution\nProbability Density / Mass Function\n(\\(x\\)) Values\n\n\n\n\nGaussian\n\\[ p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\]\n\\[ x \\in \\mathbb{R} \\]\n\n\nBernoulli\n\\[ p(x) = \\alpha^x (1 - \\alpha)^{1 - x} \\]\n\\[ x \\in \\{0, 1\\} \\]\n\n\nBinomial\n\\[ p(x) = \\binom{n}{x} \\alpha^x (1 - \\alpha)^{n - x} \\]\n\\[ x \\in \\{0, 1, 2, \\ldots, n\\} \\]\n\n\nMultinomial\n\\[ p(x) = \\frac{n!}{x_1! x_2! \\ldots x_n!} \\prod_{i=1}^{n} \\alpha_i^{x_i} \\]\n\\[ x_i \\in \\{0, 1, 2, \\ldots, n\\}, \\sum x_i = n \\]\n\n\nExponential\n\\[ p(x) = \\lambda e^{-\\lambda x} \\]\n\\[ x \\in \\mathbb{R}^+ \\]\n\n\nPoisson\n\\[ p(x) = \\frac{e^{-\\lambda} \\lambda^x}{x!} \\]\n\\[ x \\in \\{0, 1, 2, \\ldots \\} \\]\n\n\nDirichlet\n\\[ p(x) = \\frac{\\Gamma \\left( \\sum_i \\alpha_i \\right)}{\\prod_i \\Gamma(\\alpha_i)} \\prod_i x_i^{\\alpha_i - 1} \\]\n\\[ x_i \\in [0, 1], \\sum x_i = 1 \\]"
  },
  {
    "objectID": "notes/expfam/expfam.html#random-sampling",
    "href": "notes/expfam/expfam.html#random-sampling",
    "title": "The Exponential Family",
    "section": "Random sampling",
    "text": "Random sampling\nThe exponential family structure is preserved for an i.i.d. sample. If \\(\\{X_{1}, \\cdots, X_{n}\\}\\) are i.i.d. from some distribution in the exponential family \\(p(x; \\theta)\\), then the joint distribution: \\[\np(x_{1}, \\cdots, x_{n};\\theta) = \\prod_{i=1}^{n}h(x_{i})\\exp \\left[\\sum\\limits_{i=1}^{s}\\theta_{i}\\sum\\limits_{j=1}^{n}\\mathcal{T}_{i}(x_{j}) - \\eta\\mathcal{A}(\\theta)\\right]\n\\] is an exponential family with the same natural parameters but with sufficient statistics: \\[\n\\mathcal{T}_{i}(x_{1}, \\cdots, x_{n)}= \\sum\\limits_{j=1}^{n}T_i(x_{j})\n\\]"
  },
  {
    "objectID": "notes/expfam/expfam.html#log-partition-generates-moments",
    "href": "notes/expfam/expfam.html#log-partition-generates-moments",
    "title": "The Exponential Family",
    "section": "Log-partition generates moments",
    "text": "Log-partition generates moments\nTaking the derivative of \\(\\mathcal{A}\\) w.r.t. \\(\\theta\\), we obtain that:\\[\n\\begin{aligned}\n\\dfrac{d\\mathcal{A}}{d\\eta}\n&= \\dfrac{d}{d\\eta} \\log \\int \\exp(\\eta^{T} \\mathcal{T}(x))h(x) dx \\\\\n&= \\dfrac{\\dfrac{d}{d\\eta} \\int \\exp(\\eta^{T} \\mathcal{T}(x)h(x)dx}{\\int \\exp(\\eta^{T} \\mathcal{T}(x)h(x)dx} \\\\\n&= \\dfrac{\\int \\mathcal{T}(x) \\exp(\\eta^{T} \\mathcal{T}(x)h(x)dx}{\\exp(\\mathcal{A}(\\eta))} \\\\\n&= \\int \\mathcal{T}(x) \\exp(\\eta \\mathcal{T}(x) - \\mathcal{A}(\\eta))h(x)dx \\\\\n&= \\int \\mathcal{T}(x) p(x)dx = \\mathbb{E}[\\mathcal{T}(x)]\n\\end{aligned}\n\\]a concrete example, recall that for Bernoulli distribution, we have \\(\\mathcal{A}(\\eta) = \\log (1+e^{\\eta})\\), we can take the first-order derivative of the log-partition function and obtain the mean for Bernoulli random variables: \\[\n\\dfrac{d\\mathcal{A}}{d\\eta} = \\dfrac{d}{d\\eta} \\log (1+e^{\\eta}) = \\dfrac{e^{\\eta}}{1+e^{\\eta}} = \\dfrac{1}{1+e^{-\\eta}} = \\sigma(\\eta) = \\mu\n\\] Similarly, we can verify that higher derivatives lead to functions of higher moments as follows: \\[\n\\begin{aligned}\n\\dfrac{d^{2}\\mathcal{A}}{d\\eta} &= \\dfrac{d\\mathcal{A}}{d\\eta} \\int \\mathcal{T}(x)\\exp (\\eta^{T}\\mathcal{T}(x) - \\mathcal{A}(\\eta))h(x)dx \\\\\n&= \\int \\mathcal{T}(x) \\exp (\\eta^{T}\\mathcal{T}(x) - \\mathcal{A}(\\eta))h(x)(\\mathcal{T}(x) - \\mathcal{A}'(\\eta))dx \\\\\n&= \\int \\mathcal{T}(x)p(x)(\\mathcal{T}(x) - \\mathcal{A}'(x))dx \\\\\n&= \\int \\mathcal{T}^{2}(x)p(x)dx - \\mathcal{A}'(x)\\int \\mathcal{T}(x)p(x)dx \\\\\n&= \\mathbb{E}[\\mathcal{T}^{2}(x)] - \\mathbb{E}[\\mathcal{T}(x)]^{2} = \\text{Var}[\\mathcal{T}(x)]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/expfam/expfam.html#convexity",
    "href": "notes/expfam/expfam.html#convexity",
    "title": "The Exponential Family",
    "section": "Convexity",
    "text": "Convexity\nUsing Hölder inequality together with this results, we can show that \\(\\mathcal{A}\\) is a convex function of \\(\\theta\\). In the case of the exponential family, this means that the covariance matrix (equivalent to the Hessian matrix) of the sufficient statistic \\(\\mathcal{T}_{i}\\) is positive semidefinite. Formally, we have the following theorem:\n\nThe natural parameter space \\(\\mathcal{N}\\) is a convex set and the cumulant function \\(\\mathcal{A}(\\eta)\\) is a convex function. If the family is minimal then \\(\\mathcal{A}(\\eta)\\) is strictly convex.\n\nHere, we say that an exponential family is minimal if the sufficient statistic are not redundant, i.e. there is no set of coefficients \\(\\alpha \\in \\mathbb{R}^s\\), \\(\\alpha \\# 0\\) such that: \\[\n\\sum\\limits_{i=1}^{s}\\alpha_{i}\\mathcal{T}_{i}(x) = \\text{const} \\qquad \\forall x\\in \\mathbb{R}\n\\]"
  },
  {
    "objectID": "notes/expfam/expfam.html#footnotes",
    "href": "notes/expfam/expfam.html#footnotes",
    "title": "The Exponential Family",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen a distribution is consistent, it means that the values are spread out in a way that shows a clear and predictable pattern. In other words, if you take multiple samples from the distribution, they should all look similar to each other and follow the same pattern or trend. This consistency makes it easier to understand and predict the behavior of the data.↩︎\nEntropy is a measure of this unpredictability or randomness. In statistics, it tells us how mixed up or uncertain the data is. High entropy means the data is very mixed and unpredictable. Low entropy means the data is more uniform and predictable.↩︎\nWe will show later that we can actually derive the probability distributions that belong to the exponential family by solving this problem but with different constraints.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Welcome! On this page you can find a collection of my scribe notes for self-reference and learning purposes. Sometimes the notes can be woefully incomplete and so please don’t judge. There certainly are typos and other technical errors. I would really appreciate it if you could contact me via my email for further feedback and discussion.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nThe Exponential Family\n\n\n\n\n\n\nstats\n\n\n\n\n\n\nJul 10, 2024\n\n\n\n\n\n\n\nGaussian Distribution\n\n\n\n\n\n\nstats\n\n\n\n\n\n\nJul 4, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Viet Nguyen",
    "section": "",
    "text": "About me\nHi, I am currently a junior student majoring in Data Science at University of Economics Ho Chi Minh City (UEH). My research lies in multimodal models  and generative models. I am also keen to work on models’ interpretability.\n\n\nNews\n\nJuly 2024: I will join VIASM’s Summer School Series on Mathematical Statistics and Machine Learning\nApr 2024: I received the UEH Young Researcher 2024 Award.\nFeb 2024: Our paper has been accepted at Journal of Uncertain Systems.\nMar 2023: Our paper has been accepted at ICICCT2023.\nFeb 2023: I received the UEH Young Researcher 2023 Award.\nAug 2021: I was awarded the UEH Admission Scholarship.\n\n\n\nMedia Coverage\n\nUEH Data Science student published 3 Scopus-indexed papers (in Vietnamese)\n\n\n\nContact\nEmail: quocviethere [at] gmail.com"
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Performance Insights of Attention-free Language Models in Sentiment Analysis: A Case Study for E-commerce Platforms in Vietnam Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dang N.H. Thanh. 8th International Conference on Inventive Communication and Computational Technologies (ICICCT2024)\nAn Exploratory Comparison of LSTM and BiLSTM in Stock Prediction Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dinh T. Huu, Nguyen D. Toan, Dang N.H. Thanh. 7th International Conference on Inventive Communication and Computational Technologies (ICICCT2023)"
  },
  {
    "objectID": "pubs.html#conferences",
    "href": "pubs.html#conferences",
    "title": "Publications",
    "section": "",
    "text": "Performance Insights of Attention-free Language Models in Sentiment Analysis: A Case Study for E-commerce Platforms in Vietnam Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dang N.H. Thanh. 8th International Conference on Inventive Communication and Computational Technologies (ICICCT2024)\nAn Exploratory Comparison of LSTM and BiLSTM in Stock Prediction Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dinh T. Huu, Nguyen D. Toan, Dang N.H. Thanh. 7th International Conference on Inventive Communication and Computational Technologies (ICICCT2023)"
  },
  {
    "objectID": "pubs.html#journals",
    "href": "pubs.html#journals",
    "title": "Publications",
    "section": "Journals",
    "text": "Journals\nCustomer Intent Mining from Service Inquiries with Improved Deep Embedded Clustering Nguyen Q.K. Ha, Nguyen T.T. Huyen, Mai T.M. Uyen, Nguyen Q. Viet, Nguyen N. Quang, Dang N.H. Thanh. Journal of Uncertain Systems"
  },
  {
    "objectID": "notes/gaussian-distribution/gaussian-distribution.html",
    "href": "notes/gaussian-distribution/gaussian-distribution.html",
    "title": "Gaussian Distribution",
    "section": "",
    "text": "The Gaussian distribution, also known as the normal distribution, is the most extensively utilized probability distribution for real-valued random variables \\(y \\in \\mathbb{R}\\). In this article, we will delve into the definition of the Gaussian distribution and elucidate why it assumes its particular form through the principle of maximum entropy. Additionally, we will derive the normalization constant in the Gaussian probability density function (PDF)."
  },
  {
    "objectID": "notes/gaussian-distribution/gaussian-distribution.html#gaussian-distribution-derivation-from-principle-of-maximum-entropy",
    "href": "notes/gaussian-distribution/gaussian-distribution.html#gaussian-distribution-derivation-from-principle-of-maximum-entropy",
    "title": "Gaussian Distribution",
    "section": "Gaussian distribution derivation from Principle of Maximum entropy",
    "text": "Gaussian distribution derivation from Principle of Maximum entropy\nFor a continuous random variable \\(X\\) with a probability density function \\(p(x)\\), the entropy \\(h(X)\\) is defined as: \\[h(X) = -\\int_{-\\infty}^{\\infty} f(x) \\log f(x) \\, dx\\]\nMaximum entropy pdf with fixed mean \\(\\mu\\) and variance \\(\\sigma^{2}\\)\n\\[\\mathcal{L}= -\\int_{-\\infty}^{\\infty} p(x) \\log p(x)dx + \\lambda_0\\left(\\int_{-\\infty}^{\\infty} p(x)dx - 1 \\right) + \\lambda_1\\left(\\int_{-\\infty}^{\\infty}(x-\\mu)^2 p(x) dx - \\sigma^2 \\right)\\] Taking the derivative of \\(\\mathcal{L}\\) w.r.t. \\(p(x)\\), we obtain: \\[\n\\dfrac{\\partial \\mathcal{L}}{\\partial p(x)}= -(1+\\log p(x)) + \\lambda_{0}+\\lambda_{1}(x-\\mu)^{2} \\tag{4}\n\\] Setting (4) to be equal to 0, we can derive that: \\(p(x) = \\exp(\\lambda_{0}+ \\lambda(x-\\mu)^{2}-1)\\).\nWe now have to make sure that the above form of \\(p(x)\\) satisfies the initial constraints. In particular: \\[\n\\int_{-\\infty}^{\\infty} exp(\\lambda_{0}+ \\lambda(x-\\mu)^{2}-1)dx = 1 \\tag{5}\n\\] and \\[\n\\int_{-\\infty}^{\\infty}(x-\\mu)^{2}\\exp(\\lambda_{0}+ \\lambda(x-\\mu)^{2}-1)dx = \\sigma^{2} \\tag{6}\n\\] From (5) and (6), we obtain that \\(\\exp(\\lambda_{0}-1)\\sqrt{\\dfrac{-\\pi}{\\lambda_{1}}}=1\\) and \\(\\lambda_{1} = \\sqrt{\\dfrac{1}{2\\pi}}\\dfrac{1}{\\sigma}\\). Plug in everything together, we have that: \\[\n\\begin{aligned}\np(x) &= \\exp(\\lambda_{0}+ \\lambda(x-\\mu)^{2}-1) \\\\\n&= \\exp(\\lambda_{0}-1) + \\exp(\\lambda_{1}(x-\\mu)^{2})\\\\\n&= \\dfrac{1}{\\sqrt{2\\pi \\sigma^{2}}}\\exp\\left(-\\dfrac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)\n\\end{aligned}\n\\] which is exactly the form of the PDF of the Gaussian distribution."
  },
  {
    "objectID": "notes/gaussian-distribution/gaussian-distribution.html#normalizing-constant",
    "href": "notes/gaussian-distribution/gaussian-distribution.html#normalizing-constant",
    "title": "Gaussian Distribution",
    "section": "Normalizing constant",
    "text": "Normalizing constant\nIn order for one function to be a valid PDF, it has to integrate 1. Concretely, in the case of the Standard Normal PDF, as expressed in Equation 2, we have to find a constant \\(c\\) such that: \\[\nc\\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz = 1\n\\] And so we need to find the value of \\(c\\) by calculating \\(1 / \\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz\\). We start with integral: \\[\n\\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz\n\\] However, the problem is that there exist no closed-form solution to this integral. In fact, \\(\\exp\\left(\\frac{-z^{2}}{2}\\right)\\) is an example of what is known as nonelementary integral.\nSo we need to come up with another clever way to deal with this integral instead of solving it directly. We can start by writing it twice: \\[\n\\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz \\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz\n\\tag{3}\\] For the sake of convenience, we can rewrite Equation 3 as follows: \\[\n\\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-x^{2}}{2}\\right)dx \\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-y^{2}}{2}\\right)dy = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\exp\\left(-\\frac{x^{2}+y^{2}}{2}\\right)dxdy\n\\tag{4}\\]\nNow comes the “pulling the rabbit out of the hat” moment, where we’ll transform Equation 4 into Polar coordinate instead of Cartesian coordinate.\nThis is known as Change of variables (please refer to Harvard Stat110’s Math Handout as a quick reference). We can thus write the following: \\[\n\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\exp\\left(-\\frac{x^{2}+y^{2}}{2}\\right)dxdy = \\int_{0}^{2\\pi}\\int_{0}^{2\\pi} \\left[\\exp\\left(\\frac{-r^{2}}{2}\\right)r\\right] drd\\theta \\tag{3}\n\\]\nLet \\(u= \\frac{-r^{2}}{2}\\), therefore \\(du = rdr\\), we can rewrite (3) as:\n\\[\n\\int_{0}^{2\\pi}\\left(\\underbrace{\\int_{0}^{2\\pi} \\exp(-u)du}_{=1} \\right)d\\theta = \\int_{0}^{2\\pi}1 d\\theta = 2\\pi\n\\]\nRecall that we write the term \\(\\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz\\) twice (as in Equation 3), therefore \\(c = \\dfrac{1}{\\sqrt{2\\pi}}\\).\nWe can apply the exact same procedure for the Normal distribution (instead of the Standard Normal distribution) to see why \\(c = \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}\\). In fact, what we’ve just derived is the special case when \\(\\sigma^{2}=1\\)."
  }
]