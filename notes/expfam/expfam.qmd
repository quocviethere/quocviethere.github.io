---
title: "The Exponential Family"
date: 07-10-2024
categories: 
 - stats
title-block-banner: '#5b5b5b'
title-block-banner-color: white
---

# Motivation

The classical motivation for exponential family comes from the _principle of maximum entropy_. Suppose we are given a random sample $\{X_{1}, X_{2}, \cdots, X_{n}\}$ from some distribution, and we compute the empirical expectations of certain functions that we choose:
$$\hat{\mu}_{i}=\dfrac{1}{n}\sum\limits_{j=1}^{n}T_{i}(X_{j}) \quad \text{for } i \in \{1, \cdots, s\}$$
Based on these empirical expectations we want to infer a full probability distribution on the samples. A distribution $p$ is said to be *consistent* [^1] with the data we observe if:
$$
\hat{\mu}_{i} = \mathbb{E}_{p}[T_{i}(X_{i})] \quad \text{for } i \in \{1, \cdots, s\}.
$$
We want to pick a consistent distribution but there are infinitely many distributions to choose from. The **principle of maximum entropy** suggests to pick the distribution with the largest entropy [^2], which is defined as:
$$
H(p) = - \int p(x) \log p(x) dx
$$
Formally, we could pose our problem as finding a distribution $p^{*}$ such that:
$$
p^{*} = \underset{p}{\text{argmax}} H(p)
$$
Solving this problem, we get an answer that is of the form [^3]:
$$
p^{*} = \exp \left[\sum\limits_{i=1}^{s}\theta_{i}T_{i}(x) - A(\theta)\right]h(x)
$$
This form motivates the exponential families.
# The Exponential Family

## Definition

The exponential family of probability distributions is defined as a set of distributions whose density can be written in the following form:
$$
p(x \vert \eta) = h(x)\exp\{\eta^{T}\mathcal{T}(x) - \mathcal{A}(\eta)\},
$$
where:
- $\eta$ is often referred to as natural or canonical parameters
- $\mathcal{T}(x)$ is the sufficient statistic, because the likelihood for $x$ only depends on $\eta$ and $\mathcal{T}(x)$ transforms $x$ into sufficient statistic, which is something that summarizes the data well.
- $\mathcal{A}(\eta)$ is known as the cumulant function, or log-partition function and can be viewed as the logarithm of the normalization factor, i.e.: $\mathcal{A}(\eta) = \log \int h(x) \exp \{\eta^{T}\mathcal{T}(x)\}dx$

Sometimes, the exponential family can also be represented as:
$$
p(x \vert \eta) = h(x) g(\eta) \exp \{\eta^{T}\mathcal{T}(x)\} 
$$
where the $g(\eta)$ can be interpreted as the coefficient that ensures the distribution is normalized, i.e.:
$$
g(\eta) = \int h(x) \exp\{\eta^{T}\mathcal{T}(x)\}
$$
## Examples

The simplest possible distribution that belongs to the exponential family is the Bernoulli distribution. Recall that we can write the Bernoulli distribution as:
$$
p(x \vert \mu) = \text{Bern}(x \vert \mu) = \mu^{x}(1-\mu)^{1-x}
$$
which we can rewrite as:
$$
p(x \vert \mu) = \exp\{x \ln \mu + (1-x)\ln (1-\mu)\} = (1-\mu) \exp \left\{x\ln \left(\dfrac{\mu}{1-\mu}\right) + \ln (1-\mu)\right\}  
$$
We can therefore see that the Bernoulli distribution is in the exponential family distribution where $\mathcal{T}(x) = x; \eta = \ln \left(\dfrac{\mu}{1-\mu}\right)$. One interesting fact is that when we solve for $\mu$, we obtain the well-known *logistic function*. The derivation is as follows:
$$
\begin{aligned}
\eta &= \ln \left(\dfrac{\mu}{1-\mu}\right) && (\text{derived from rewriting Bernoulli distribution})\\
& \rightarrow \exp(\eta) = \dfrac{\mu}{1-\mu} && \text{(taking exp on both sides)}\\
& \rightarrow \mu = \dfrac{\exp(\eta)}{1+\exp(\eta)} && \text{(by basic algebra)}\\
& \rightarrow \mu = \dfrac{1}{1+\exp(-\eta)} && \text{(multiplying by $\exp(-\mu)$)}
\end{aligned}
$$
There are many common probability distribution that is in the exponential family. The table below listed some of them.

| Distribution | Probability Density / Mass Function                                                                            | ($x$) Values                                       |
| ------------ | -------------------------------------------------------------------------------------------------------------- | -------------------------------------------------- |
| Gaussian     | $$ p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} $$                                  | $$ x \in \mathbb{R} $$                             |
| Bernoulli    | $$ p(x) = \alpha^x (1 - \alpha)^{1 - x} $$                                                                     | $$ x \in \{0, 1\} $$                               |
| Binomial     | $$ p(x) = \binom{n}{x} \alpha^x (1 - \alpha)^{n - x} $$                                                        | $$ x \in \{0, 1, 2, \ldots, n\} $$                 |
| Multinomial  | $$ p(x) = \frac{n!}{x_1! x_2! \ldots x_n!} \prod_{i=1}^{n} \alpha_i^{x_i} $$                                   | $$ x_i \in \{0, 1, 2, \ldots, n\}, \sum x_i = n $$ |
| Exponential  | $$ p(x) = \lambda e^{-\lambda x} $$                                                                            | $$ x \in \mathbb{R}^+ $$                           |
| Poisson      | $$ p(x) = \frac{e^{-\lambda} \lambda^x}{x!} $$                                                                 | $$ x \in \{0, 1, 2, \ldots \} $$                   |
| Dirichlet    | $$ p(x) = \frac{\Gamma \left( \sum_i \alpha_i \right)}{\prod_i \Gamma(\alpha_i)} \prod_i x_i^{\alpha_i - 1} $$ | $$ x_i \in [0, 1], \sum x_i = 1 $$                 |

# Properties of the Exponential Family

## Random sampling

The exponential family structure is preserved for an i.i.d. sample. If $\{X_{1}, \cdots, X_{n}\}$ are i.i.d. from some distribution in the exponential family $p(x; \theta)$, then the joint distribution:
$$
p(x_{1}, \cdots, x_{n};\theta) = \prod_{i=1}^{n}h(x_{i})\exp \left[\sum\limits_{i=1}^{s}\theta_{i}\sum\limits_{j=1}^{n}\mathcal{T}_{i}(x_{j}) - \eta\mathcal{A}(\theta)\right]
$$
is an exponential family with the same natural parameters but with sufficient statistics:
$$
\mathcal{T}_{i}(x_{1}, \cdots, x_{n)}= \sum\limits_{j=1}^{n}T_i(x_{j})
$$

## Log-partition generates moments

Taking the derivative of $\mathcal{A}$ w.r.t. $\theta$, we obtain that:$$
\begin{aligned}
\dfrac{d\mathcal{A}}{d\eta} 
&= \dfrac{d}{d\eta} \log \int \exp(\eta^{T} \mathcal{T}(x))h(x) dx \\ 
&= \dfrac{\dfrac{d}{d\eta} \int \exp(\eta^{T} \mathcal{T}(x)h(x)dx}{\int \exp(\eta^{T} \mathcal{T}(x)h(x)dx} \\
&= \dfrac{\int \mathcal{T}(x) \exp(\eta^{T} \mathcal{T}(x)h(x)dx}{\exp(\mathcal{A}(\eta))} \\
&= \int \mathcal{T}(x) \exp(\eta \mathcal{T}(x) - \mathcal{A}(\eta))h(x)dx \\
&= \int \mathcal{T}(x) p(x)dx = \mathbb{E}[\mathcal{T}(x)]
\end{aligned}
$$\As a concrete example, recall that for Bernoulli distribution, we have $\mathcal{A}(\eta) = \log (1+e^{\eta})$, we can take the first-order derivative of the log-partition function and obtain the mean for Bernoulli random variables:
$$
\dfrac{d\mathcal{A}}{d\eta} = \dfrac{d}{d\eta} \log (1+e^{\eta}) = \dfrac{e^{\eta}}{1+e^{\eta}} = \dfrac{1}{1+e^{-\eta}} = \sigma(\eta) = \mu
$$
Similarly, we can verify that higher derivatives lead to functions of higher moments as follows:
$$
\begin{aligned}
\dfrac{d^{2}\mathcal{A}}{d\eta} &= \dfrac{d\mathcal{A}}{d\eta} \int \mathcal{T}(x)\exp (\eta^{T}\mathcal{T}(x) - \mathcal{A}(\eta))h(x)dx \\
&= \int \mathcal{T}(x) \exp (\eta^{T}\mathcal{T}(x) - \mathcal{A}(\eta))h(x)(\mathcal{T}(x) - \mathcal{A}'(\eta))dx \\
&= \int \mathcal{T}(x)p(x)(\mathcal{T}(x) - \mathcal{A}'(x))dx \\
&= \int \mathcal{T}^{2}(x)p(x)dx - \mathcal{A}'(x)\int \mathcal{T}(x)p(x)dx \\
&= \mathbb{E}[\mathcal{T}^{2}(x)] - \mathbb{E}[\mathcal{T}(x)]^{2} = \text{Var}[\mathcal{T}(x)]
\end{aligned}
$$

## Convexity

Using Hölder inequality together with this results, we can show that $\mathcal{A}$ is a convex function of $\theta$. In the case of the exponential family, this means that the covariance matrix (equivalent to the Hessian matrix) of the sufficient statistic $\mathcal{T}_{i}$ is positive semidefinite. Formally, we have the following theorem:

>
>*The natural parameter space $\mathcal{N}$ is a convex set and the cumulant function $\mathcal{A}(\eta)$ is a convex function. If the family is minimal then $\mathcal{A}(\eta)$ is strictly convex.*

Here, we say that an exponential family is minimal if the sufficient statistic are not *redundant*, i.e. there is no set of coefficients $\alpha \in \mathbb{R}^s$, $\alpha \# 0$ such that:
$$
\sum\limits_{i=1}^{s}\alpha_{i}\mathcal{T}_{i}(x) = \text{const} \qquad \forall x\in \mathbb{R}
$$

# Connection with Fisher information matrix

Under some regularity condition, the Fisher Information matrix is defined as:
$$
\mathbf{F}(\eta) \triangleq \mathbb{E}_{p(x\vert \eta)} [\nabla \log p(x \vert \eta) \nabla \log p(x \vert \eta)^{T}] = -\mathbb{E}_{p(x \vert \eta)} [\nabla^{2}\log p(x\vert \eta)] 
$$
Recall that in the context of the exponential family, we have 

From the definition of the Fisher Information matrix, we can derive the following:
$$
\begin{aligned}
\mathbf{F}(\eta) &= -\mathbb{E}_{p(x\vert \eta)}[\nabla^{2}\log p(x\vert \eta)] && \text{(by definition)}\\
&= -\mathbb{E}_{p(x \vert \eta)}[\nabla^{2}_{\eta}(\eta^{T} \mathcal{T}(x) - \mathcal{A(\eta)})] && \text{(plug in $p(x \vert \eta)$)} \\
&= \nabla^{2}_{\eta}\mathcal{A} (\eta) = \text{Cov}[\mathcal{T}(x)]
\end{aligned}
$$
Intuitively, this is telling us that the Hessian of the log-partition function is the same as the Fisher Information matrix, which is the same as the covariance of the sufficient statistic.

# References

- [Lecture Note of 36-705 Intermediate Statistics by Prof. Larry Wasserman](https://www.stat.cmu.edu/~larry/=stat705/Lecture12a.pdf)
- [The exponential family: Basics](https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf)
- Murphy, K. P. (2023). _Probabilistic Machine Learning: Advanced Topics_. MIT Press. [http://probml.github.io/book2](http://probml.github.io/book2)

# Further Reading

- [Information Theory and Statistics, John C. Duchi. Lecture Notes for Statistics 311/Electrical Engineering 377, Stanford University. 2015-2019.](https://web.stanford.edu/class/stats311/lecture-notes.pdf#page=46.19)

---
[^1]: When a distribution is consistent, it means that the values are spread out in a way that shows a clear and predictable pattern. In other words, if you take multiple samples from the distribution, they should all look similar to each other and follow the same pattern or trend. This consistency makes it easier to understand and predict the behavior of the data.

[^2]: **Entropy** is a measure of this unpredictability or randomness. In statistics, it tells us how mixed up or uncertain the data is. High entropy means the data is very mixed and unpredictable. Low entropy means the data is more uniform and predictable.

[^3]: We will show later that we can actually derive the probability distributions that belong to the exponential family by solving this problem but with different constraints.
