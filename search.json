[
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Performance Insights of Attention-free Language Models in Sentiment Analysis: A Case Study for E-commerce Platforms in Vietnam Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dang N.H. Thanh. 8th International Conference on Inventive Communication and Computational Technologies (ICICCT2024)\nAn Exploratory Comparison of LSTM and BiLSTM in Stock Prediction Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dinh T. Huu, Nguyen D. Toan, Dang N.H. Thanh. 7th International Conference on Inventive Communication and Computational Technologies (ICICCT2023)"
  },
  {
    "objectID": "pubs.html#conferences",
    "href": "pubs.html#conferences",
    "title": "Publications",
    "section": "",
    "text": "Performance Insights of Attention-free Language Models in Sentiment Analysis: A Case Study for E-commerce Platforms in Vietnam Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dang N.H. Thanh. 8th International Conference on Inventive Communication and Computational Technologies (ICICCT2024)\nAn Exploratory Comparison of LSTM and BiLSTM in Stock Prediction Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dinh T. Huu, Nguyen D. Toan, Dang N.H. Thanh. 7th International Conference on Inventive Communication and Computational Technologies (ICICCT2023)"
  },
  {
    "objectID": "pubs.html#journals",
    "href": "pubs.html#journals",
    "title": "Publications",
    "section": "Journals",
    "text": "Journals\nCustomer Intent Mining from Service Inquiries with Improved Deep Embedded Clustering Nguyen Q.K. Ha, Nguyen T.T. Huyen, Mai T.M. Uyen, Nguyen Q. Viet, Nguyen N. Quang, Dang N.H. Thanh. Journal of Uncertain Systems"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Viet Nguyen",
    "section": "",
    "text": "About me\nHi, I am currently a junior student majoring in Data Science at University of Economics Ho Chi Minh City (UEH). My research lies in multimodal models  and generative models. I’m also keen to work on models’ interpretability.\n\n\nNews\n\nApr 2024: I received the UEH Young Researcher 2024 Award.\nFeb 2024: Our paper has been accepted at Journal of Uncertain Systems.\nMar 2023: Our paper has been accepted at ICICCT - 2023.\nFeb 2023: I received the UEH Young Researcher 2023 Award.\nAug 2021: I was awarded the UEH Admission Scholarship.\n\n\n\nMedia Coverage\n\nUEH Data Science student published 3 Scopus-indexed papers (in Vietnamese)\n\n\n\nContact\nEmail: quocviethere [at] gmail.com"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nGaussian distribution\n\n\n\n\n\n\nstats\n\n\n\nThe most widely-used probability distribution for continuous random variables.\n\n\n\n\n\nJul 3, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notes/gaussian-distribution/gaussian-distribution.html",
    "href": "notes/gaussian-distribution/gaussian-distribution.html",
    "title": "Gaussian distribution",
    "section": "",
    "text": "date: 2024-07-03"
  },
  {
    "objectID": "notes/gaussian-distribution/gaussian-distribution.html#normalizing-constant",
    "href": "notes/gaussian-distribution/gaussian-distribution.html#normalizing-constant",
    "title": "Gaussian distribution",
    "section": "Normalizing constant",
    "text": "Normalizing constant\nIn order for one function to be a valid PDF, it has to integrate 1. Concretely, in the case of the Standard Normal PDF, we have to find a constant \\(c\\) such that: \\[\nc\\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz = 1\n\\] And so we need to find the value of \\(c\\) by calculating \\(1 / \\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz\\). We start with integral: \\[\n\\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz\n\\] However, the problem is that there exist no closed-form solution to this integral. In fact, \\(\\exp\\left(\\frac{-z^{2}}{2}\\right)\\) is an example of what is known as nonelementary integral.\nSo we need to come up with another clever way to deal with this integral instead of solving it directly. We can start by writing it twice:\n\\[\n\\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz \\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz\n\\tag{1}\\]\nFor the sake of convenience, we can rewrite (Equation 1) as follows: \\[\n\\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-x^{2}}{2}\\right)dx \\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-y^{2}}{2}\\right)dy = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\exp\\left(-\\frac{x^{2}+y^{2}}{2}\\right)dxdy\n\\tag{2}\\] Now comes the “pulling the rabbit out of the hat” moment, where we’ll transform (Equation 2) into Polar ordinate instead of Cartesian coordinate.\n\nThis is known as Change of variables (please refer to Harvard Stat110’s Math Handout as a quick reference). We can thus write the following: \\[\n\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\exp\\left(-\\frac{x^{2}+y^{2}}{2}\\right)dxdy = \\int_{0}^{2\\pi}\\int_{0}^{2\\pi} \\left[\\exp\\left(\\frac{-r^{2}}{2}\\right)r\\right] drd\\theta\n\\tag{3}\\] Let \\(u= \\frac{-r^{2}}{2}\\), therefore \\(du = rdr\\), we can rewrite (Equation 3) as: \\[\n\\int_{0}^{2\\pi}\\left(\\underbrace{\\int_{0}^{2\\pi} \\exp(-u)du}_{=1} \\right)d\\theta = \\int_{0}^{2\\pi}1 d\\theta = 2\\pi\n\\] Recall that we write the term \\(\\int_{-\\infty}^{\\infty}\\exp \\left(\\frac{-z^{2}}{2}\\right)dz\\) twice, as in (Equation 1), therefore \\(c = \\dfrac{1}{\\sqrt{2\\pi}}\\).\nWe can apply the exact same procedure for the Normal distribution (instead of the Standard Normal distribution) to see why \\(c = \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}\\). In fact, what we’ve just derived is the special case when \\(\\sigma^{2}=1\\)."
  },
  {
    "objectID": "notes/gaussian-distribution/gaussian-distribution.html#gaussian-distribution-derivation-from-principle-of-maximum-entropy",
    "href": "notes/gaussian-distribution/gaussian-distribution.html#gaussian-distribution-derivation-from-principle-of-maximum-entropy",
    "title": "Gaussian distribution",
    "section": "Gaussian distribution derivation from Principle of Maximum entropy",
    "text": "Gaussian distribution derivation from Principle of Maximum entropy\nTo understand why the Gaussian distribution has the density of the form \\(c\\exp \\left(-\\dfrac{1}{2\\sigma^{2}} (y-\\mu)^{2}\\right)\\), one way to look at it is through Principle of Maximum Entropy.\nRecall that the Entropy is defined as the measure of uncertainty. The higher the entropy of a random variable \\(X\\), the more uncertainty it incorporates.We can formally define entropy as:\n\n\n\n\n\n\nEntropy\n\n\n\nFor a continuous random variable \\(X\\) with a probability density function \\(p(x)\\), the entropy \\(h(X)\\) is defined as: \\[h(X) = -\\int_{-\\infty}^{\\infty} f(x) \\log f(x) \\, dx\\]\n\n\nThe Principle of Maximum Entropy is based on the premise that the best probability distribution is the one that has the largest entropy (i.e. the largest amount of uncertainty) subject to the given constraints. In the case of the Gaussian distribution, the two constraints are:\n\n\\(\\int p(x)dx = 1\\)\n\\(\\int_{-\\infty}^{\\infty}(x-\\mu)^2 p(x) dx = \\sigma^2\\) 1.\n\nWe can now formalize our problem as finding the optimal probability distribution \\(p^*\\) that maximizes the entropy with subject to the two constraints, i.e.\n\\[\np^* = \\underset{p}{\\text{argmax}}\\int_{-\\infty}^{\\infty} p(x) \\log p(x) dx\n\\]\ns.t. \\(\\int p(x)dx = 1\\) and \\(\\int_{-\\infty}^{\\infty}(x-\\mu)^2 p(x) dx = \\sigma^2\\).\nTo solve this problem, we can rewrite it in the form of the Lagrangian equation as:\n\\[\\mathcal{L}= -\\int_{-\\infty}^{\\infty} p(x) \\log p(x)dx + \\lambda_0\\left(\\int_{-\\infty}^{\\infty} p(x)dx - 1 \\right) + \\lambda_1\\left(\\int_{-\\infty}^{\\infty}(x-\\mu)^2 p(x) dx - \\sigma^2 \\right)\\] Taking the derivative of \\(\\mathcal{L}\\) w.r.t. \\(p(x)\\), we obtain: \\[\n\\dfrac{\\partial \\mathcal{L}}{\\partial p(x)}= -(1+\\log p(x)) + \\lambda_{0}+\\lambda_{1}(x-\\mu)^{2}\n\\tag{4}\\]\nSetting (Equation 4) to be equal to 0, we can derive that: \\(p(x) = \\exp(\\lambda_{0}+ \\lambda(x-\\mu)^{2}-1)\\).\nWe now have to make sure that the above form of \\(p(x)\\) satisfies the initial constraints. In particular: \\[\n\\int_{-\\infty}^{\\infty} exp(\\lambda_{0}+ \\lambda(x-\\mu)^{2}-1)dx = 1\n\\tag{5}\\] and \\[\n\\int_{-\\infty}^{\\infty}(x-\\mu)^{2}\\exp(\\lambda_{0}+ \\lambda(x-\\mu)^{2}-1)dx = \\sigma^{2}\n\\tag{6}\\] From (Equation 5) and (Equation 6), we obtain that \\(\\exp(\\lambda_{0}-1)\\sqrt{\\dfrac{-\\pi}{\\lambda_{1}}}=1\\) and \\(\\lambda_{1} = \\sqrt{\\dfrac{1}{2\\pi}}\\dfrac{1}{\\sigma}\\). Plug in everything together, we have that: \\[\n\\begin{aligned}\np(x) &= \\exp(\\lambda_{0}+ \\lambda(x-\\mu)^{2}-1) \\\\\n&= \\exp(\\lambda_{0}-1) + \\exp(\\lambda_{1}(x-\\mu)^{2})\\\\\n&= \\dfrac{1}{\\sqrt{2\\pi \\sigma^{2}}}\\exp\\left(-\\dfrac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)\n\\end{aligned}\n\\] which is exactly the form of the PDF of the Gaussian distribution."
  },
  {
    "objectID": "notes/gaussian-distribution/gaussian-distribution.html#footnotes",
    "href": "notes/gaussian-distribution/gaussian-distribution.html#footnotes",
    "title": "Gaussian distribution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nnote that \\(\\mu\\) is already in \\(\\sigma^2\\) so we only need one constraint.↩︎"
  }
]